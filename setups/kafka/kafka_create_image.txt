    gcloud compute addresses create kafka-broker-01-ip \
    --region ${region} --subnet ${vpc_subnet} --addresses 10.65.0.13
    
    gcloud compute addresses create kafka-broker-02-ip \
    --region ${region} --subnet ${vpc_subnet} --addresses 10.65.0.14
    
    gcloud compute addresses create kafka-broker-03-ip \
    --region ${region} --subnet ${vpc_subnet} --addresses 10.65.0.15
    
    gcloud compute instances create kafka-broker-01 \
    --async \
    --boot-disk-size 100GB \
    --image centos-7-v20230203 \
    --image-project centos-cloud \
    --machine-type ${kafka_machine_type} \
    --private-network-ip 10.65.0.13 \
    --scopes=https://www.googleapis.com/auth/cloud-platform \
    --subnet ${vpc_subnet} \
    --tags kafka \
    --labels=application=kafka \
    --zone=${region_zone_a} \
    --no-address
    
gcloud beta compute ssh kafka-broker-01 \
    --tunnel-through-iap \
    --zone=${region_zone_a}
    
## install jdk
sudo yum update 
sudo yum install java-11-openjdk-devel
sudo yum install wget
# Add a user
sudo useradd kafka -m && sudo usermod --shell /bin/bash kafka
#set a password for kafka 
sudo passwd kafka
# Add kafka user to sudoers 
sudo usermod -aG wheel kafka
# make a dir
sudo mkdir -p /data/kafka && sudo mkdir -p /data/kafka/logs
# Download binaries 
sudo wget https://dlcdn.apache.org/kafka/3.4.0/kafka_2.12-3.4.0.tgz
sudo tar -xvzf kafka_2.12-3.4.0.tgz --strip 1
sudo chown -R kafka:kafka /data/kafka

sudo mv /data/kafka/config/server.properties /data/kafka/config/server.properties-OLD
vim /data/kafka/config/server.properties #use above config from the gist


#Change the broker id 
broker.id=0
#Change the internal IP
listeners=PLAINTEXT://10.65.0.13:9092
advertised.listeners=PLAINTEXT://10.65.0.13:9092
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=2147483647
log.dirs=/data/kafka/logs/
num.partitions=5
log.retention.hours=6
log.segment.bytes=2147483647
log.retention.check.interval.ms=300000

############################# Zookeeper #############################
zookeeper.connect=zk-01.us-east4-a.c.on-prem-project-337210.internal:2181,zk-02.us-east4-b.c.on-prem-project-337210.internal:2181,zk-03.us-east4-c.c.on-prem-project-337210.internal:2181
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000

#######################################################
group.initial.rebalance.delay.ms=0
message.max.bytes=2147483647
max.message.bytes=2147483647
reserved.broker.max.id=10000000
auto.leader.rebalance.enable=true
delete.topic.enable=true
default.replication.factor=3
replica.lag.time.max.ms=100000
replica.fetch.wait.max.ms=5000
queued.max.requests=8192
num.replica.fetchers=6
num.network.threads=16
num.recovery.threads.per.data.dir=3
num.io.threads=16
replica.socket.receive.buffer.bytes=2147483647
replica.fetch.max.bytes=2147483647
message.timestamp.type=LogAppendTime
group.max.session.timeout.ms=7200000


sudo vim /etc/systemd/system/kafka.service


[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target
[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/data/kafka/bin/kafka-server-start.sh /data/kafka/config/server.properties > /data/kafka/kafka.log 2>&1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal
[Install]
WantedBy=multi-user.target


systemctl start kafka

bin/zkCli.sh -server 127.0.0.1:2181

ls /brokers/ids

gcloud beta compute machine-images \
  create kafka-machine-image \
  --source-instance kafka-broker-01 \
  --source-instance-zone ${region_zone_a}
